{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building fastText word embedding\n",
    "https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings?scriptVersionId=2456848"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method normalize to clean and prepare a single string. We will use it later to prepare our string data. Also, we load the data as we're used to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastText import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of words we look at per example. Experiment with this.\n",
    "window_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Replace ips\n",
    "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    # some cleaning \n",
    "    s = re.sub(r\"what's\", \"what is \", s)\n",
    "    s = re.sub(r\"\\'s\", \" \", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have \", s)\n",
    "    s = re.sub(r\"can't\", \"cannot \", s)\n",
    "    s = re.sub(r\"n't\", \" not \", s)\n",
    "    s = re.sub(r\"i'm\", \"i am \", s)\n",
    "    s = re.sub(r\"\\'re\", \" are \", s)\n",
    "    s = re.sub(r\"\\'d\", \" would \", s)\n",
    "    s = re.sub(r\"\\'ll\", \" will \", s)\n",
    "    s = re.sub(r\"\\'scuse\", \" excuse \", s)\n",
    "    s = re.sub('\\W', ' ', s)\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    s = s.strip(' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading data')\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "train['comment_text'] = train['comment_text'].fillna('_empty_')\n",
    "test['comment_text'] = test['comment_text'].fillna('_empty_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok next, let's load the FastText model and define methods that convert text to a sequence of vectors. Note that I'm just considering the last n words of each text. You could play with this, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FT model\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading FT model')\n",
    "ft_model = load_model('../input/fastText.model.bin')\n",
    "n_features = ft_model.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Given a string, normalizes it, then splits it into words and finally converts\n",
    "    it to a sequence of word vectors.\n",
    "    \"\"\"\n",
    "    text = normalize(text)\n",
    "    words = text.split()\n",
    "    window = words[-window_length:]\n",
    "    \n",
    "    x = np.zeros((window_length, n_features))\n",
    "\n",
    "    for i, word in enumerate(window):\n",
    "        x[i, :] = ft_model.get_word_vector(word).astype('float32')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_data(df):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    x = np.zeros((len(df), window_length, n_features), dtype='float32')\n",
    "\n",
    "    for i, comment in enumerate(df['comment_text'].values):\n",
    "        x[i, :] = text_to_vector(comment)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert an input dataframe to an input vector, just call df_to_data. This will result in the shape (n_examples, window_length, n_features). Here, for each row we would have 100 words a 300 features each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset:\n",
    "split_index = round(len(train) * 0.9)\n",
    "shuffled_train = train.sample(frac=1)\n",
    "df_train = shuffled_train.iloc[:split_index]\n",
    "df_val = shuffled_train.iloc[split_index:]\n",
    "\n",
    "# Convert validation set to fixed array\n",
    "x_val = df_to_data(df_val)\n",
    "y_val = df_val[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['toxic' 'severe_toxic' 'obscene' 'threat' 'insult' 'identity_hate'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2746dfa5b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['toxic' 'severe_toxic' 'obscene' 'threat' 'insult' 'identity_hate'] not in index\""
     ]
    }
   ],
   "source": [
    "x_test = df_to_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df, batch_size):\n",
    "    \"\"\"\n",
    "    Given a raw dataframe, generates infinite batches of FastText vectors.\n",
    "    \"\"\"\n",
    "    batch_i = 0 # Counter inside the current batch vector\n",
    "    batch_x = None # The current batch's x data\n",
    "    batch_y = None # The current batch's y data\n",
    "    \n",
    "    while True: # Loop forever\n",
    "        df = df.sample(frac=1) # Shuffle df each epoch\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            comment = row['comment_text']\n",
    "            \n",
    "            if batch_x is None:\n",
    "                batch_x = np.zeros((batch_size, window_length, n_features), dtype='float32')\n",
    "                batch_y = np.zeros((batch_size, len(classes)), dtype='float32')\n",
    "                \n",
    "            batch_x[batch_i] = text_to_vector(comment)\n",
    "            batch_y[batch_i] = row[classes].values\n",
    "            batch_i += 1\n",
    "\n",
    "            if batch_i == batch_size:\n",
    "                # Ready to yield the batch\n",
    "                yield batch_x, batch_y\n",
    "                batch_x = None\n",
    "                batch_y = None\n",
    "                batch_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1), input_shape=(window_length, n_features)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(6, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()  # TODO: Implement\n",
    "\n",
    "batch_size = 128\n",
    "training_steps_per_epoch = round(len(df_train) / batch_size)\n",
    "training_generator = data_generator(df_train, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you should be good to go! Train this as usual. You don't need an EmbeddingLayer, but you need to pass input_shape=(window_length, n_features) to the first layer in your NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1122/1122 [==============================] - 1871s 2s/step - loss: 0.0784 - acc: 0.9747 - val_loss: 0.0511 - val_acc: 0.9815\n",
      "Epoch 2/2\n",
      "1122/1122 [==============================] - 1826s 2s/step - loss: 0.0503 - acc: 0.9816 - val_loss: 0.0480 - val_acc: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a973d160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ready to start training:\n",
    "model.fit_generator(\n",
    "    training_generator,\n",
    "    steps_per_epoch=training_steps_per_epoch,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 867s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([x_test], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "sample_submission[classes] = y_test\n",
    "sample_submission.to_csv('submission_fastText.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
